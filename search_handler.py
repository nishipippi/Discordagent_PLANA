# search_handler.py
# (æ¤œç´¢ã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†ã€Brave Search APIé€£æºã€URLãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º)

import asyncio
import re
import httpx
import discord
import json # DSRCãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã§ä½¿ã†å¯èƒ½æ€§ (ä»Šå›ã¯ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã ãŒå°†æ¥çš„ã«æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚‚è€ƒæ…®)
from typing import List, Dict, Any, Optional, Tuple, Literal, Union # Unionã‚’è¿½åŠ 

import config
import bot_constants
import llm_manager
import cache_manager # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã®ãŸã‚è¿½åŠ 
import discord_ui # Thinking message, ãƒœã‚¿ãƒ³ç”Ÿæˆç”¨
from llm_provider import ERROR_TYPE_UNKNOWN # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—å®šæ•°

# command_handler ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…¨ä½“ã§ã¯ãªãã€handle_mention é–¢æ•°ã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# from command_handler import handle_mention # <- handle_mention é–¢æ•°ã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ãŸã ã—ã€assess_and_respond_to_mention é–¢æ•°ã¯ command_handler ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…¨ä½“ã‚’å‚ç…§ã—ã¦ã„ã‚‹ãŸã‚ã€
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…¨ä½“ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç¶­æŒã—ã¤ã¤ã€Pylanceã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã„ã‚ˆã†ã«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ä½¿ã†æ–¹æ³•ã‚’è©¦ã¿ã¾ã™ã€‚
# ã‚‚ã—ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§ãƒ€ãƒ¡ãªã‚‰ã€handle_mention ã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å‘¼ã³å‡ºã—ç®‡æ‰€ã‚’å¤‰æ›´ã—ã¾ã™ã€‚
import command_handler as ch # <- ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ


# --- Brave Search API Call ---
async def call_brave_search_api(query: str) -> Optional[List[Dict[str, Any]]]:
    """Brave Search APIã‚’å‘¼ã³å‡ºã™"""
    if not config.BRAVE_SEARCH_API_KEY:
        print("Error: BRAVE_SEARCH_API_KEY is not set.")
        return None

    headers = {
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "X-Subscription-Token": config.BRAVE_SEARCH_API_KEY,
        "User-Agent": "PlanaBot/1.0 (Discord Bot)" # é©åˆ‡ãªUser-Agent
    }
    params = {
        "q": query,
        "count": config.MAX_SEARCH_RESULTS,
        "search_filter": "web",
        # "country": "jp", # å¿…è¦ã«å¿œã˜ã¦
        # "search_lang": "ja" # å¿…è¦ã«å¿œã˜ã¦
    }

    async with httpx.AsyncClient() as client:
        try:
            print(f"Calling Brave Search API for query: '{query}'...")
            response = await client.get(config.BRAVE_SEARCH_API_URL, headers=headers, params=params, timeout=20)
            response.raise_for_status()
            data = response.json()
            results = data.get('web', {}).get('results', [])
            print(f"Brave Search API call successful. Found {len(results)} web results.")
            return results
        except httpx.HTTPStatusError as e:
            print(f"HTTP error occurred while calling Brave Search API: {e.response.status_code}")
            # print(f"Response body: {e.response.text}") # ãƒ‡ãƒãƒƒã‚°ç”¨
            return None
        except httpx.RequestError as e:
            print(f"An error occurred while requesting Brave Search API: {e}")
            return None
        except Exception as e:
            print(f"An unexpected error occurred during Brave Search API call: {e}")
            return None
        finally:
            # APIå‘¼ã³å‡ºã—ã”ã¨ã«å¿…ãšå¾…æ©Ÿ (try/except/finallyã§ä¿è¨¼)
            await asyncio.sleep(config.BRAVE_API_DELAY)


# --- URL Content Extraction ---
async def extract_text_from_url(url: str) -> Optional[str]:
    """URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹ (ç°¡æ˜“ç‰ˆ)"""
    if not url or not (url.startswith("http://") or url.startswith("https://")):
        print(f"Invalid URL skipped: {url}")
        return None

    print(f"Attempting to extract text from URL: {url}")
    try:
        # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§Content-Typeã‚’ç¢ºèª
        async with httpx.AsyncClient(timeout=10, follow_redirects=True) as client:
            try:
                 head_response = await client.head(url)
                 head_response.raise_for_status()
                 content_type = head_response.headers.get('Content-Type', '').lower()
                 # text/html, text/plain, application/json ä»¥å¤–ã‚’ã‚¹ã‚­ãƒƒãƒ—
                 if not any(ct in content_type for ct in ['text/html', 'text/plain', 'application/json']):
                     print(f"Skipping non-HTML/text/JSON content type: {content_type} for {url}")
                     return None
            except httpx.HTTPStatusError as e:
                 print(f"HEAD request failed for {url}: {e.response.status_code}")
                 return None # HEADå¤±æ•—ã¯ã‚¢ã‚¯ã‚»ã‚¹ä¸èƒ½ã¨ã¿ãªã™
            except httpx.RequestError as e:
                 print(f"HEAD request failed for {url}: {e}")
                 return None

        # GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
        async with httpx.AsyncClient(timeout=30, follow_redirects=True) as client:
             response = await client.get(url)
             response.raise_for_status()
             content_type = response.headers.get('Content-Type', '').lower() # å†åº¦å–å¾—

             # Content-Typeã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
             if 'application/json' in content_type:
                  try:
                      # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã—ã€ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’çµåˆï¼ˆç°¡æ˜“çš„ï¼‰
                      json_data = response.json()
                      text_parts = []
                      def extract_text_from_json(data):
                          if isinstance(data, dict):
                              for key, value in data.items():
                                  if isinstance(value, str):
                                      text_parts.append(value)
                                  else:
                                      extract_text_from_json(value)
                          elif isinstance(data, list):
                              for item in data:
                                  extract_text_from_json(item)
                          elif isinstance(data, str):
                              text_parts.append(data)
                      extract_text_from_json(json_data)
                      text_content = ' '.join(text_parts).strip()
                  except Exception as json_e:
                       print(f"Failed to parse JSON or extract text from {url}: {json_e}. Falling back to raw text.")
                       text_content = response.text # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
             elif 'text/html' in content_type:
                 html_content = response.text # httpxãŒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¨å®š
                 # HTMLã‚¿ã‚°é™¤å» (ç°¡æ˜“ç‰ˆ)
                 text_content = re.sub(r'<script.*?>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
                 text_content = re.sub(r'<style.*?>.*?</style>', '', text_content, flags=re.DOTALL | re.IGNORECASE)
                 text_content = re.sub(r'<!--.*?-->', '', text_content, flags=re.DOTALL)
                 text_content = re.sub(r'>\s*<', '> <', text_content) # ã‚¿ã‚°é–“ã®ç©ºç™½
                 text_content = re.sub(r'<.*?>', '', text_content) # å…¨ã‚¿ã‚°é™¤å»
             else: # text/plain ãªã©
                  text_content = response.text

             text_content = re.sub(r'\s+', ' ', text_content).strip() # é€£ç¶šç©ºç™½ã‚’ã¾ã¨ã‚ã‚‹

             # é•·ã•ãƒã‚§ãƒƒã‚¯ã¨åˆ‡ã‚Šè©°ã‚
             if len(text_content) > config.MAX_CONTENT_LENGTH_PER_URL:
                 text_content = text_content[:config.MAX_CONTENT_LENGTH_PER_URL] + "..."
                 print(f"Truncated content for {url} to {config.MAX_CONTENT_LENGTH_PER_URL} characters.")

             # çŸ­ã™ãã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã‚¹ã‚­ãƒƒãƒ—
             if len(text_content) < config.SEARCH_MIN_CONTENT_LENGTH:
                 print(f"Content too short ({len(text_content)} chars) for {url}. Skipping.")
                 return None

             print(f"Successfully extracted text from {url} ({len(text_content)} chars).")
             return text_content

    except httpx.HTTPStatusError as e:
        print(f"HTTP error occurred while fetching URL {url}: {e.response.status_code}")
        return None
    except httpx.RequestError as e:
        print(f"An error occurred while requesting URL {url}: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred while processing URL {url}: {e}")
        import traceback
        traceback.print_exc()
        return None


# --- Search Necessity Assessment ---
async def should_perform_search(question: str) -> bool:
    """ä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦æ¤œç´¢ãŒå¿…è¦ã‹ Lowload ãƒ¢ãƒ‡ãƒ«ã§åˆ¤æ–­ã™ã‚‹"""
    llm_handler = llm_manager.get_current_provider()
    if not llm_handler:
        print("Warning: LLM handler not available for search necessity assessment.")
        return False # LLMåˆ©ç”¨ä¸å¯ãªã‚‰æ¤œç´¢ã§ããªã„/åˆ¤æ–­ä¸å¯

    lowload_model_name = llm_handler.get_model_name('lowload')
    if not lowload_model_name:
        print(f"Warning: Lowload model unavailable ({llm_manager.get_current_provider_name()}) for search necessity assessment.")
        return False # ä½è² è·ãƒ¢ãƒ‡ãƒ«ãŒãªã‘ã‚Œã°åˆ¤æ–­ä¸å¯ -> æ¤œç´¢ã—ãªã„

    try:
        assessment_prompt = config.SEARCH_NECESSITY_ASSESSMENT_PROMPT.format(question=question)
        # Lowload ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        assessment_response_raw = await llm_manager.generate_lowload_response(assessment_prompt)
        assessment_response = str(assessment_response_raw).strip().lower() if assessment_response_raw else ""

        print(f"Search necessity assessment for '{question[:50]}...': Response='{assessment_response}'")
        # å¿œç­”ãŒ "å¿…è¦" ã¨å®Œå…¨ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿ True
        # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ã‚ˆã†ã« lower() ã—ã¦ã‹ã‚‰æ¯”è¼ƒ
        return assessment_response == "å¿…è¦"

    except Exception as e:
        print(f"Error during search necessity assessment: {e}")
        import traceback
        traceback.print_exc()
        return False # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã—ã¦æ¤œç´¢ã—ãªã„


# --- Mention Response with Search Assessment ---
async def assess_and_respond_to_mention(message: discord.Message, question_text: str):
    """
    ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å¿œç­”æ™‚ã«æ¤œç´¢ãŒå¿…è¦ã‹åˆ¤æ–­ã—ã€
    å¿…è¦ãªã‚‰ !src ç›¸å½“ã®æ¤œç´¢ã‚’å®Ÿè¡Œã€ä¸è¦ãªã‚‰ command_handler.handle_mention ã‚’å‘¼ã³å‡ºã™
    """
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚‹ãŸã‚ã€Thinkingãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒãƒ£ãƒ³ãƒãƒ«ã¯æ˜ç¤ºçš„ã«æ¸¡ã›ã‚‹
    await discord_ui.update_thinking_message(message.channel, "â€¦æ¤œç´¢ãŒå¿…è¦ã‹åˆ¤æ–­ä¸­...")

    needs_search = await should_perform_search(question_text)

    if needs_search:
        print("Search deemed necessary by LLM. Performing simple search (!src equivalent).")
        # perform_search=True ã¯ä¸è¦ã ãŒã€ã‚³ãƒ¼ãƒ‰ã®æ„å›³ã‚’æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã«æ®‹ã™
        # handle_search_command ã«å‡¦ç†ã‚’ç§»è­²ã—ã€Thinking Message ã¯å¼•ãç¶™ãŒã‚Œã‚‹
        await handle_search_command(message, 'src', question_text, triggered_by_assessment=True)
    else:
        print("Search deemed unnecessary by LLM. Proceeding with standard mention response.")
        await discord_ui.update_thinking_message(message.channel, "â€¦æ¤œç´¢ä¸è¦ã¨åˆ¤æ–­ã€å¿œç­”æº–å‚™ä¸­...")
        # Thinking Message ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰é€šå¸¸ã®ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å¿œç­”ã‚’å‘¼ã³å‡ºã™
        await discord_ui.delete_thinking_message()
        # é€šå¸¸ã®ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å¿œç­” (æ¤œç´¢ãªã—)
        # message.guild.me ãŒ None ã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰æ¸¡ã™
        if message.guild and message.guild.me:
             # command_handler ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ä½¿ç”¨ã—ã¦ handle_mention ã‚’å‘¼ã³å‡ºã—
             await ch.handle_mention(message, message.guild.me, question_text=question_text, perform_search=False)
        else:
             print("Error: Cannot get bot user info (message.guild.me) in assess_and_respond_to_mention.")
             await message.reply(bot_constants.ERROR_MSG_INTERNAL + " (Bot user info not found)", mention_author=False)


# --- Deep Search (!dsrc) Core Logic ---

async def generate_dsrc_plan(question: str) -> Optional[List[str]]:
    """!dsrc ã®ãŸã‚ã®èª¿æŸ»è¨ˆç”»ã‚’ç”Ÿæˆã™ã‚‹"""
    llm_handler = llm_manager.get_current_provider()
    primary_model_name = llm_manager.get_active_model_name('primary')
    if not llm_handler or not primary_model_name:
        print("Error: Primary model unavailable for DSRC plan generation.")
        return None

    plan_prompt = config.DSRC_PLAN_GENERATION_PROMPT.format(
        question=question, max_steps=config.DSRC_MAX_PLAN_STEPS
    )
    try:
        # generate_response ã¯ãƒ¢ãƒ‡ãƒ«åã¨å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ (Primaryãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
        _used_model, plan_response_raw = await llm_manager.generate_response(
            content_parts=[{'text': plan_prompt}], chat_history=None, deep_cache_summary=None
        )
        plan_response = str(plan_response_raw).strip() if plan_response_raw else ""

        if not plan_response or llm_manager.is_error_message(plan_response):
            print(f"DSRC Plan generation failed. Response: {plan_response}")
            return None

        # ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹ (ç°¡æ˜“çš„)
        plan_steps_raw = [line.strip() for line in plan_response.splitlines() if line.strip()]
        # ç•ªå·ã‚’é™¤å» (ä¾‹: "1. ", "2. ")
        plan_steps = [re.sub(r"^\s*\d+\.\s*", "", step) for step in plan_steps_raw] # å…ˆé ­ã®ç©ºç™½ã¨ç•ªå·ã‚’é™¤å»
        plan_steps = [step for step in plan_steps if step] # ç©ºã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’é™¤å»

        if not plan_steps:
             print("DSRC Plan generation resulted in empty steps.")
             return None

        print(f"DSRC Plan Generated ({len(plan_steps)} steps):")
        for i, step in enumerate(plan_steps): print(f"  {i+1}. {step}")
        return plan_steps[:config.DSRC_MAX_PLAN_STEPS] # æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«åˆ¶é™

    except Exception as e:
        print(f"Error during DSRC plan generation: {e}")
        import traceback
        traceback.print_exc()
        return None


async def assess_dsrc_step_results(question: str, step_description: str, search_results_text: str) -> Tuple[str, Optional[str]]:
    """!dsrc ã®ç‰¹å®šã®ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’è©•ä¾¡ã™ã‚‹"""
    llm_handler = llm_manager.get_current_provider()
    primary_model_name = llm_manager.get_active_model_name('primary')
    if not llm_handler or not primary_model_name:
        print("Error: Primary model unavailable for DSRC step assessment.")
        return "ERROR", "Primary model unavailable."

    assessment_prompt = config.DSRC_STEP_ASSESSMENT_PROMPT.format(
        question=question,
        step_description=step_description,
        search_results_text=search_results_text
    )
    try:
        # generate_response ã¯ãƒ¢ãƒ‡ãƒ«åã¨å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ (Primaryãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
        _used_model, assessment_response_raw = await llm_manager.generate_response(
            content_parts=[{'text': assessment_prompt}], chat_history=None, deep_cache_summary=None
        )
        assessment_response = str(assessment_response_raw).strip() if assessment_response_raw else ""

        if not assessment_response or llm_manager.is_error_message(assessment_response):
            print(f"DSRC Step assessment failed. Response: {assessment_response}")
            return "ERROR", f"Assessment failed: {assessment_response}"

        # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ã‚ˆã†ã« upper() ã—ã¦ã‹ã‚‰æ¯”è¼ƒ
        if assessment_response.upper() == 'COMPLETE':
            return "COMPLETE", None
        elif assessment_response.upper().startswith('INCOMPLETE:'):
            missing_info = assessment_response.split(':', 1)[1].strip() if ':' in assessment_response else "è©³ç´°ä¸æ˜"
            return "INCOMPLETE", missing_info
        else:
            # äºˆæœŸã›ã¬å½¢å¼ -> ä¸å®Œå…¨ã¨ã¿ãªã—ã€å¿œç­”å†…å®¹ã‚’ä¸è¶³æƒ…å ±ã¨ã™ã‚‹
            print(f"Warning: Unexpected DSRC step assessment format: '{assessment_response}'. Treating as INCOMPLETE.")
            return "INCOMPLETE", assessment_response

    except Exception as e:
        print(f"Error during DSRC step assessment: {e}")
        import traceback
        traceback.print_exc()
        return "ERROR", f"Exception during assessment: {e}"


async def execute_dsrc_step(
    question: str,
    step_description: str,
    step_index: int,
    all_results_so_far: Dict[str, str] # ã“ã‚Œã¾ã§ã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã§é›†ã‚ãŸçµæœ
    ) -> Tuple[Dict[str, str], List[Dict[str, Any]]]:
    """!dsrc ã®1ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ (æœ€å¤§Nå›ã®æ¤œç´¢ãƒ»è©•ä¾¡ã‚µã‚¤ã‚¯ãƒ«)"""
    llm_handler = llm_manager.get_current_provider()
    primary_model_name = llm_manager.get_active_model_name('primary')
    if not llm_handler or not primary_model_name:
        print(f"Error executing DSRC Step {step_index+1}: Primary model unavailable.")
        return {}, [{"step": step_index + 1, "status": "ERROR", "reason": "Primary model unavailable", "queries": [], "results": {}}]

    step_results: Dict[str, str] = {} # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æ–°ãŸã«è¦‹ã¤ã‹ã£ãŸçµæœ (URL -> text)
    step_assessments: List[Dict[str, Any]] = [] # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®è©•ä¾¡å±¥æ­´
    used_queries_for_step: List[str] = [] # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ä½¿ã£ãŸã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
    missing_info: Optional[str] = None # å‰å›ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ä¸è¶³ã—ã¦ã„ãŸæƒ…å ±

    for iteration in range(config.DSRC_MAX_ITERATIONS_PER_STEP):
        iteration_label = f"ã‚¹ãƒ†ãƒƒãƒ— {step_index+1} ({iteration+1}/{config.DSRC_MAX_ITERATIONS_PER_STEP}å›ç›®)"
        # discord_ui.update_thinking_message ã‚’å‘¼ã¶éš›ã« channel ãŒå¿…é ˆã§ãªã„ã‚ˆã†ã« discord.utils.MISSING ã‚’æ¸¡ã™
        await discord_ui.update_thinking_message(discord.utils.MISSING, f"â€¦è€ƒãˆä¸­... ({iteration_label} ã‚¯ã‚¨ãƒªç”Ÿæˆä¸­)")

        # 1. æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ
        query_gen_prompt = config.DSRC_STEP_QUERY_GENERATION_PROMPT.format(
            question=question,
            step_description=step_description,
            used_queries_for_step=", ".join(used_queries_for_step) or "ãªã—",
            missing_info=missing_info if missing_info else "ç‰¹ã«æŒ‡å®šãªã—" # Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ–‡å­—åˆ—
        )
        try:
            # generate_response ã¯ãƒ¢ãƒ‡ãƒ«åã¨å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ (Primaryãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
            _used_model_q, query_response_raw = await llm_manager.generate_response(
                 content_parts=[{'text': query_gen_prompt}], chat_history=None, deep_cache_summary=None
            )
            query_response_text = str(query_response_raw).strip() if query_response_raw else ""
            if not query_response_text or llm_manager.is_error_message(query_response_text):
                 print(f"[{iteration_label}] Query generation failed. Response: {query_response_text}")
                 # ã‚¯ã‚¨ãƒªç”Ÿæˆå¤±æ•—ã¯ã‚¹ãƒ†ãƒƒãƒ—ç¶šè¡Œä¸å¯ã¨ã¿ãªã—ã€ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦çµ‚äº†
                 step_assessments.append({"step": step_index + 1, "iteration": iteration + 1, "status": "ERROR", "reason": f"Query generation failed: {query_response_text}", "queries": [], "results": {}})
                 return step_results, step_assessments # ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—ã§çµ‚äº†

            queries_raw = query_response_text.replace('\n', ',')
            current_iteration_queries = [q.strip().strip('"') for q in queries_raw.split(',') if q.strip()]
            current_iteration_queries = [q for q in current_iteration_queries if q]
            current_iteration_queries = current_iteration_queries[:3] # æœ€å¤§3ã¤

            if not current_iteration_queries:
                 print(f"[{iteration_label}] Generated empty query list. Proceeding to assessment with existing results.")
                 # ã‚¯ã‚¨ãƒªãŒç”Ÿæˆã•ã‚Œãªãã¦ã‚‚ã€æ—¢å­˜ã®çµæœã§è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚ºã«é€²ã‚€
                 pass # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸
            else:
                 # æ–°ã—ã„ã‚¯ã‚¨ãƒªã®ã¿ used_queries_for_step ã«è¿½åŠ 
                 new_queries = [q for q in current_iteration_queries if q not in used_queries_for_step]
                 if new_queries: used_queries_for_step.extend(new_queries)
                 print(f"[{iteration_label}] Generated queries: {current_iteration_queries}")


        except Exception as e:
            print(f"[{iteration_label}] Error during query generation: {e}")
            import traceback
            traceback.print_exc()
            step_assessments.append({"step": step_index + 1, "iteration": iteration + 1, "status": "ERROR", "reason": f"Query generation exception: {e}", "queries": [], "results": {}})
            return step_results, step_assessments # ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—ã§çµ‚äº†


        # 2. Brave Search & å†…å®¹å–å¾—
        current_iteration_extracted: Dict[str, str] = {} # ã“ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å–å¾—ã—ãŸæ–°ã—ã„çµæœ
        if current_iteration_queries: # ã‚¯ã‚¨ãƒªãŒã‚ã‚‹å ´åˆã®ã¿æ¤œç´¢
             search_results_api: List[Dict[str, Any]] = []
             for query in current_iteration_queries:
                  await discord_ui.update_thinking_message(discord.utils.MISSING, f"â€¦è€ƒãˆä¸­... ({iteration_label} æ¤œç´¢ä¸­: `{query[:30]}...`)")
                  results = await call_brave_search_api(query)
                  if results: search_results_api.extend(results)

             unique_urls = list(dict.fromkeys([r['url'] for r in search_results_api if 'url' in r]))
             # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ä»Šå›ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¾ã å–å¾—ã—ã¦ã„ãªã„URLã€ã‹ã¤å…¨ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚‚ã¾ã å–å¾—ã—ã¦ã„ãªã„URL
             urls_to_fetch = [url for url in unique_urls if url not in step_results and url not in all_results_so_far]

             if urls_to_fetch:
                  await discord_ui.update_thinking_message(discord.utils.MISSING, f"â€¦è€ƒãˆä¸­... ({iteration_label} ãƒšãƒ¼ã‚¸å†…å®¹å–å¾—ä¸­ {len(urls_to_fetch)}ä»¶)")
                  fetch_tasks = [extract_text_from_url(url) for url in urls_to_fetch]
                  extracted_contents_list = await asyncio.gather(*fetch_tasks)
                  for url, content in zip(urls_to_fetch, extracted_contents_list):
                      if content:
                          step_results[url] = content # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã«è¿½åŠ 
                          current_iteration_extracted[url] = content # ã“ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å–å¾—ã—ãŸã‚‚ã®
                  print(f"[{iteration_label}] Extracted content from {len(current_iteration_extracted)}/{len(urls_to_fetch)} new URLs.")
             else:
                 print(f"[{iteration_label}] No new unique URLs to fetch in this iteration.")


        # 3. è©•ä¾¡
        await discord_ui.update_thinking_message(discord.utils.MISSING, f"â€¦è€ƒãˆä¸­... ({iteration_label} çµæœè©•ä¾¡ä¸­)")
        # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§é›†ã‚ãŸå…¨çµæœï¼ˆéå»ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å«ã‚€ï¼‰ã§è©•ä¾¡
        combined_step_results_text = "\n\n".join(
            f"--- Content from {url} ---\n{text}\n--- End of {url} ---"
            for url, text in step_results.items() # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…¨çµæœ
        )
        # è©•ä¾¡ç”¨ã«é•·ã•ã‚’åˆ‡ã‚Šè©°ã‚ã‚‹ (configã®å€¤ã‚’ä½¿ç”¨)
        if len(combined_step_results_text) > config.MAX_TOTAL_SEARCH_CONTENT_LENGTH:
            combined_step_results_text = combined_step_results_text[:config.MAX_TOTAL_SEARCH_CONTENT_LENGTH] + "\n\n... (truncated for assessment)"

        status, assessment_detail = await assess_dsrc_step_results(question, step_description, combined_step_results_text)

        # è©•ä¾¡çµæœã‚’è¨˜éŒ²
        step_assessments.append({
            "step": step_index + 1,
            "iteration": iteration + 1,
            "status": status,
            "reason": assessment_detail,
            "queries": current_iteration_queries, # ã“ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ä½¿ã£ãŸã‚¯ã‚¨ãƒª
            "results": current_iteration_extracted # ã“ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å–å¾—ã—ãŸæ–°ã—ã„çµæœ
        })
        print(f"[{iteration_label}] Assessment: {status} - {assessment_detail}")

        if status == "COMPLETE":
            print(f"Step {step_index+1} completed.")
            return step_results, step_assessments # ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†
        elif status == "ERROR":
             print(f"Error during assessment in Step {step_index+1}. Stopping step.")
             return step_results, step_assessments # ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—
        elif status == "INCOMPLETE":
             missing_info = assessment_detail # æ¬¡ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã«ä¸è¶³æƒ…å ±ã‚’æ›´æ–°
             if iteration == config.DSRC_MAX_ITERATIONS_PER_STEP - 1:
                  print(f"Max iterations reached ({config.DSRC_MAX_ITERATIONS_PER_STEP}) for Step {step_index+1}. Proceeding with incomplete results.")
                  return step_results, step_assessments # æœ€å¤§å›æ•°è©¦è¡Œã—ã¦ã‚‚å®Œäº†ã›ãšçµ‚äº†
             # else: ãƒ«ãƒ¼ãƒ—ç¶šè¡Œ

    # ã“ã“ã«åˆ°é”ã™ã‚‹ã®ã¯é€šå¸¸ã€æœ€å¤§åå¾©å›æ•°ã‚’è¶…ãˆãŸå ´åˆ
    print(f"Step {step_index+1} finished after max iterations.")
    return step_results, step_assessments


async def generate_dsrc_report(question: str, plan: List[str], all_step_results: Dict[str, str], all_assessments: List[Dict[str, Any]]) -> Optional[str]:
    """!dsrc ã®æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    llm_handler = llm_manager.get_current_provider()
    primary_model_name = llm_manager.get_active_model_name('primary')
    lowload_model_name = llm_manager.get_active_model_name('lowload') # Lowloadãƒ¢ãƒ‡ãƒ«åã‚‚å–å¾—

    if not llm_handler or not primary_model_name:
        print("Error: Primary model unavailable for DSRC report generation.")
        return None

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆç”¨ã®æƒ…å ±ã‚’æ•´å½¢
    plan_text = "\n".join(f"{i+1}. {step}" for i, step in enumerate(plan))

    # å…¨æ¤œç´¢çµæœãƒ†ã‚­ã‚¹ãƒˆ (URL + content) ã‚’çµåˆ
    combined_search_results_text = "\n\n".join(
        f"--- Content from {url} ---\n{text}\n--- End of {url} ---"
        for url, text in all_step_results.items()
    )

    # --- æ¤œç´¢çµæœãŒé•·ã™ãã‚‹å ´åˆã®è¦ç´„å‡¦ç† ---
    report_input_results_text = combined_search_results_text # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
    report_input_source = "full results" # ãƒ¬ãƒãƒ¼ãƒˆå…¥åŠ›ãŒå…ƒã®çµæœã‹è¦ç´„ã‹ã‚’ç¤ºã™ãƒ©ãƒ™ãƒ«
    source_urls_list = list(all_step_results.keys()) # å…ƒã®URLãƒªã‚¹ãƒˆã¯å¸¸ã«ä¿æŒ

    # è¨­å®šã•ã‚ŒãŸæœ€å¤§å…¥åŠ›æ–‡å­—æ•°ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if len(combined_search_results_text) > config.MAX_INPUT_CHARS_FOR_SUMMARY:
        print(f"DSRC report input (all_results_text) length ({len(combined_search_results_text)}) exceeds summary limit ({config.MAX_INPUT_CHARS_FOR_SUMMARY}). Attempting to summarize using lowload model.")

        if lowload_model_name:
            await discord_ui.update_thinking_message(discord.utils.MISSING, f"â€¦è€ƒãˆä¸­... (æ¤œç´¢çµæœã‚’è¦ç´„ä¸­ using {lowload_model_name})")
            summarize_prompt = config.SUMMARIZE_SEARCH_RESULTS_PROMPT.format(
                question=question,
                search_results_text=combined_search_results_text # é•·ã„å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™
            )
            try:
                # Lowload ãƒ¢ãƒ‡ãƒ«ã§è¦ç´„ã‚’è©¦ã¿ã‚‹
                summarized_results_raw = await llm_manager.generate_lowload_response(summarize_prompt)
                summarized_results_text = str(summarized_results_raw).strip() if summarized_results_raw else ""

                # è¦ç´„ãŒæˆåŠŸã—ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ãªã„ã‹ã€ãŠã‚ˆã³ã€Œè¦ç´„ã§ãã¾ã›ã‚“ã§ã—ãŸã€ã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                if summarized_results_text and not llm_manager.is_error_message(summarized_results_text) and "è¦ç´„ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚" not in summarized_results_text:
                    print(f"Successfully summarized search results ({len(summarized_results_text)} chars).")
                    report_input_results_text = f"ã€åé›†ã•ã‚ŒãŸæ¤œç´¢çµæœã®è¦ç´„ã€‘\n{summarized_results_text}" # è¦ç´„ã§ã‚ã‚‹ã“ã¨ã‚’æ˜è¨˜
                    report_input_source = "summarized results"
                else:
                    print(f"Lowload model failed to summarize search results. Response: {summarized_results_text}. Using truncated full results.")
                    # è¦ç´„å¤±æ•—æ™‚ã¯ã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¼·åˆ¶çš„ã«åˆ‡ã‚Šè©°ã‚ã¦ä½¿ç”¨
                    report_input_results_text = combined_search_results_text[:config.MAX_INPUT_CHARS_FOR_SUMMARY] + "\n\n... (Full results truncated for report generation due to length or summary failure)"
                    report_input_source = "truncated full results"

            except Exception as e:
                print(f"Error during search results summarization: {e}. Using truncated full results.")
                import traceback
                traceback.print_exc()
                # è¦ç´„ä¸­ã«ä¾‹å¤–ç™ºç”Ÿæ™‚ã‚‚ã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¼·åˆ¶çš„ã«åˆ‡ã‚Šè©°ã‚ã¦ä½¿ç”¨
                report_input_results_text = combined_search_results_text[:config.MAX_INPUT_CHARS_FOR_SUMMARY] + "\n\n... (Full results truncated for report generation due to exception)"
                report_input_source = "truncated full results (exception)"
        else:
             print("Lowload model not available for summarization. Using truncated full results.")
             # Lowload ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã‚‚ã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¼·åˆ¶çš„ã«åˆ‡ã‚Šè©°ã‚ã¦ä½¿ç”¨
             report_input_results_text = combined_search_results_text[:config.MAX_INPUT_CHARS_FOR_SUMMARY] + "\n\n... (Full results truncated for report generation, lowload model unavailable)"
             report_input_source = "truncated full results (lowload missing)"

    # --- å…¨è©•ä¾¡çµæœãƒ†ã‚­ã‚¹ãƒˆ ---
    assessments_summary_lines = []
    for assessment in all_assessments:
         line = f"- Step {assessment['step']} (Iter {assessment['iteration']}): Status={assessment['status']}"
         # reason ãŒ None ã§ãªã„ã“ã¨ã‚’ç¢ºèª
         if assessment.get('reason') is not None:
              line += f", Reason={str(assessment['reason'])[:100]}..." # é•·ã™ãã‚‹ç†ç”±ã‚’çœç•¥
         if assessment.get('queries'): line += f", Queries={assessment['queries']}"
         # results ã¯ãƒ†ã‚­ã‚¹ãƒˆé‡ãŒå¤šã„ã®ã§çœç•¥ã™ã‚‹ã‹ã€URLã ã‘ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
         # results ãŒ None ã§ãªã„ã“ã¨ã‚’ç¢ºèª
         if assessment.get('results') is not None:
              line += f", New Results URLs={list(assessment['results'].keys())}"
         assessments_summary_lines.append(line)
    all_assessments_text = "\n".join(assessments_summary_lines)


    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    report_prompt = config.DSRC_FINAL_REPORT_PROMPT.format(
        question=question,
        plan=plan_text,
        all_results_text=report_input_results_text, # ã“ã“ã§è¦ç´„ã¾ãŸã¯åˆ‡ã‚Šè©°ã‚ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
        all_assessments_text=all_assessments_text # è©•ä¾¡ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
    )

    try:
        await discord_ui.update_thinking_message(discord.utils.MISSING, f"â€¦è€ƒãˆä¸­... (æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ using {primary_model_name}, input: {report_input_source})")
        # Primary ãƒ¢ãƒ‡ãƒ«ã§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        _used_model, report_response_raw = await llm_manager.generate_response(
            content_parts=[{'text': report_prompt}], chat_history=None, deep_cache_summary=None
        )
        report_response = str(report_response_raw).strip() if report_response_raw else ""

        if not report_response or llm_manager.is_error_message(report_response):
            print(f"DSRC Report generation failed. Response: {report_response}")
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå¤±æ•—æ™‚ã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
            return f"{bot_constants.ERROR_MSG_INTERNAL} (æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå¤±æ•—)\nReason: {report_response}"

        print("DeepResearch Final Report generated successfully.")

        # --- æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã«ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’è¿½åŠ  ---
        source_header = "**å‚ç…§ã‚½ãƒ¼ã‚¹:**"
        # LLMãŒã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’å«ã‚ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ (ç°¡æ˜“)
        # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å°æ–‡å­—ã«ã—ã¦ã‹ã‚‰æ¤œç´¢
        if source_header.lower() not in report_response.lower():
             print("LLM did not include sources in the final report. Appending manually.")
             if source_urls_list:
                  source_list_text = "\n".join([f"- <{url}>" for url in source_urls_list])
                  report_response += f"\n\n{source_header}\n{source_list_text}"
             else:
                  report_response += f"\n\n{source_header}\n(ã‚½ãƒ¼ã‚¹ãªã—)" # ã‚½ãƒ¼ã‚¹ãŒãªã„å ´åˆ

        return report_response

    except Exception as e:
        print(f"Error during DSRC report generation: {e}")
        import traceback
        traceback.print_exc() # è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
        return f"{bot_constants.ERROR_MSG_INTERNAL} (æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e})"


# --- Search Command Handler ---
async def handle_search_command(
        message: discord.Message,
        command_type: Literal['src', 'dsrc'],
        query_text: str, # <- question_text ã§ã¯ãªã query_text ã‚’ä½¿ç”¨
        triggered_by_assessment: bool = False # assess_and_respond_to_mention ã‹ã‚‰å‘¼ã°ã‚ŒãŸã‹
    ):
    """!src ãŠã‚ˆã³ !dsrc ã‚³ãƒãƒ³ãƒ‰ã€ã¾ãŸã¯è‡ªå‹•æ¤œç´¢ã®å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©"""

    # APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯ãªã©
    if not config.BRAVE_SEARCH_API_KEY:
        await message.reply("æ¤œç´¢æ©Ÿèƒ½ã¯è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (APIã‚­ãƒ¼ä¸è¶³)ã€‚", mention_author=False)
        # Thinking Message ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§å‰Šé™¤
        await discord_ui.delete_thinking_message()
        return
    llm_handler = llm_manager.get_current_provider()
    if not llm_handler:
        await message.reply(bot_constants.ERROR_MSG_INTERNAL + " (LLM Provider not available)", mention_author=False)
        # Thinking Message ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§å‰Šé™¤
        await discord_ui.delete_thinking_message()
        return

    original_question = query_text.strip() # <- query_text ã‚’ä½¿ç”¨
    if not original_question:
        command_display = f"è‡ªå‹•æ¤œç´¢ ({command_type})" if triggered_by_assessment else f"!{command_type}"
        await message.reply(f"æ¤œç´¢ã™ã‚‹å†…å®¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ä¾‹: `@{message.guild.me.display_name} {command_display} å†…å®¹`", mention_author=False)
        # Thinking Message ãŒæ®‹ã£ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§å‰Šé™¤
        await discord_ui.delete_thinking_message()
        return

    provider_name = llm_manager.get_current_provider_name()
    search_source = "Assessment" if triggered_by_assessment else f"!{command_type.upper()}"
    print(f"[{search_source}] Search process started for: '{original_question}' by {message.author.display_name} (Provider: {provider_name})")

    # æ€è€ƒä¸­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–‹å§‹ (assess_and_respond_to_mention ã‹ã‚‰å‘¼ã°ã‚ŒãŸå ´åˆã¯æ—¢ã«è¡¨ç¤ºæ¸ˆã¿)
    if not triggered_by_assessment:
        thinking_msg_prefix = f"â€¦è€ƒãˆä¸­... ({search_source})"
        await discord_ui.update_thinking_message(message.channel, f"{thinking_msg_prefix} é–‹å§‹")

    final_sent_message: Optional[discord.Message] = None # é€ä¿¡ã—ãŸæœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    final_response_text = "" # æœ€çµ‚çš„ãªLLMã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    all_extracted_content: Dict[str, str] = {} # åé›†ã—ãŸå…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (URL -> text)

    try:
        # --- !src ã¾ãŸã¯ è‡ªå‹•æ¤œç´¢ ã®å ´åˆ ---
        if command_type == 'src':
            model_type = 'lowload'
            query_model_name = llm_manager.get_active_model_name(model_type)
            answer_model_name = query_model_name
            if not query_model_name:
                 await discord_ui.delete_thinking_message()
                 await message.reply(bot_constants.ERROR_MSG_LOWLOAD_UNAVAILABLE + f" ({provider_name} ã«ä½è² è·ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“)", mention_author=False); return

            thinking_msg_prefix = f"â€¦è€ƒãˆä¸­... ({search_source})" # å†è¨­å®š (assessmentã‹ã‚‰ã®å¼•ç¶™ãã‚‚è€ƒæ…®)
            await discord_ui.update_thinking_message(message.channel, f"{thinking_msg_prefix} ã‚¯ã‚¨ãƒªç”Ÿæˆä¸­ ({query_model_name})")

            # 1. ã‚¯ã‚¨ãƒªç”Ÿæˆ (Lowloadãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
            query_gen_prompt = config.SEARCH_QUERY_GENERATION_PROMPT.format(question=original_question)
            # generate_lowload_response ã‚’ä½¿ç”¨
            query_response_raw = await llm_manager.generate_lowload_response(query_gen_prompt)
            query_response_text = str(query_response_raw).strip() if query_response_raw else ""
            if not query_response_text or llm_manager.is_error_message(query_response_text):
                 await discord_ui.delete_thinking_message(); await message.reply("æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆå¤±æ•—ã€‚", mention_author=False); return

            queries_raw = query_response_text.replace('\n', ',')
            search_queries = [q.strip().strip('"') for q in queries_raw.split(',') if q.strip()][:3]
            if not search_queries: await discord_ui.delete_thinking_message(); await message.reply("æœ‰åŠ¹ãªæ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆå¤±æ•—ã€‚", mention_author=False); return
            print(f"[{search_source}] Generated queries: {search_queries}")

            # 2. Brave Search & å†…å®¹å–å¾—
            search_results_api: List[Dict[str, Any]] = []
            for query in search_queries:
                 await discord_ui.update_thinking_message(message.channel, f"{thinking_msg_prefix} æ¤œç´¢ä¸­: `{query[:30]}...`")
                 results = await call_brave_search_api(query)
                 if results: search_results_api.extend(results)
                 # call_brave_search_api å†…ã§é…å»¶

            unique_urls = list(dict.fromkeys([r['url'] for r in search_results_api if 'url' in r]))
            if unique_urls:
                await discord_ui.update_thinking_message(message.channel, f"{thinking_msg_prefix} ãƒšãƒ¼ã‚¸å†…å®¹å–å¾—ä¸­ {len(unique_urls)}ä»¶")
                fetch_tasks = [extract_text_from_url(url) for url in unique_urls]
                extracted_contents_list = await asyncio.gather(*fetch_tasks)
                for url, content in zip(unique_urls, extracted_contents_list):
                    if content: all_extracted_content[url] = content # å…¨ä½“çµæœã«é›†ç´„
                print(f"[{search_source}] Extracted content from {len(all_extracted_content)}/{len(unique_urls)} URLs.")

            if not all_extracted_content: await discord_ui.delete_thinking_message(); await message.reply("æ¤œç´¢çµæœã‹ã‚‰æœ‰åŠ¹ãªæƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", mention_author=False); return

            # 3. æœ€çµ‚å¿œç­”ç”Ÿæˆ (Lowloadãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
            await discord_ui.update_thinking_message(message.channel, f"{thinking_msg_prefix} å¿œç­”ç”Ÿæˆä¸­ ({answer_model_name})")
            # LLMã«æ¸¡ã™çµåˆçµæœãƒ†ã‚­ã‚¹ãƒˆ (srcã§ã¯è¦ç´„ã—ãªã„ãŒã€æœ€å¤§é•·ã§åˆ‡ã‚Šè©°ã‚ã‚‹)
            combined_results_text_for_llm = "\n\n".join(f"--- {url} ---\n{text}\n--- End ---" for url, text in all_extracted_content.items())
            if len(combined_results_text_for_llm) > config.MAX_TOTAL_SEARCH_CONTENT_LENGTH: # configã®å€¤ã‚’å†åˆ©ç”¨
                 combined_results_text_for_llm = combined_results_text_for_llm[:config.MAX_TOTAL_SEARCH_CONTENT_LENGTH] + "...(truncated)"

            answer_prompt = config.SIMPLE_SEARCH_ANSWER_PROMPT.format(question=original_question, search_results_text=combined_results_text_for_llm)
            # generate_lowload_response ã‚’ä½¿ç”¨
            final_response_raw = await llm_manager.generate_lowload_response(answer_prompt) # Lowloadãƒ¢ãƒ‡ãƒ«
            final_response_text = str(final_response_raw).strip() if final_response_raw else ""

            if not final_response_text or llm_manager.is_error_message(final_response_text):
                 await discord_ui.delete_thinking_message(); await message.reply(f"å¿œç­”ç”Ÿæˆå¤±æ•—: {final_response_text}", mention_author=False); return

            response_header = f"(ğŸ” **Search Result** using {answer_model_name} ğŸ”)\n\n"

        # --- !dsrc ã®å ´åˆ ---
        elif command_type == 'dsrc':
            primary_model_name = llm_manager.get_active_model_name('primary')
            if not primary_model_name:
                 await discord_ui.delete_thinking_message(); await message.reply(f"{bot_constants.ERROR_MSG_API_ERROR} ({provider_name} ã«Primaryãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“)", mention_author=False); return

            # all_extracted_content ã¯ãƒ«ãƒ¼ãƒ—å¤–ã§åˆæœŸåŒ–æ¸ˆã¿
            all_assessments: List[Dict[str, Any]] = [] # å…¨ã‚¹ãƒ†ãƒƒãƒ—ã®è©•ä¾¡çµæœ

            thinking_msg_prefix = f"â€¦è€ƒãˆä¸­... ({search_source})" # å†è¨­å®š

            # 1. è¨ˆç”»ç”Ÿæˆ
            await discord_ui.update_thinking_message(message.channel, f"{thinking_msg_prefix} èª¿æŸ»è¨ˆç”»ç”Ÿæˆä¸­ ({primary_model_name})")
            plan = await generate_dsrc_plan(original_question)
            if not plan: await discord_ui.delete_thinking_message(); await message.reply("èª¿æŸ»è¨ˆç”»ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚", mention_author=False); return
            print(f"[{search_source}] Generated Plan: {plan}") # ãƒ­ã‚°ã«å‡ºåŠ›

            # 2. å„ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
            for i, step_description in enumerate(plan):
                print(f"--- Executing DSRC Step {i+1}: {step_description} ---")
                # å®Ÿè¡Œå‰ã«thinking messageæ›´æ–° (channelã‚’æ¸¡ã™)
                await discord_ui.update_thinking_message(message.channel, f"{thinking_msg_prefix} ã‚¹ãƒ†ãƒƒãƒ— {i+1}/{len(plan)} å®Ÿè¡Œä¸­: {step_description[:30]}...")

                step_results, step_assessments = await execute_dsrc_step(
                    original_question, step_description, i, all_extracted_content # ã“ã‚Œã¾ã§ã®å…¨çµæœã‚’æ¸¡ã™
                )
                all_extracted_content.update(step_results) # æ–°ã—ã„çµæœã‚’å…¨ä½“ã®çµæœã«è¿½åŠ 
                all_assessments.extend(step_assessments) # æ–°ã—ã„è©•ä¾¡ã‚’å…¨ä½“è©•ä¾¡ã«è¿½åŠ 

                # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ (assessment ã® status ãŒ ERROR)
                if any(a['status'] == 'ERROR' for a in step_assessments if a.get('step') == i+1): # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¨ãƒ©ãƒ¼ã®ã¿ãƒã‚§ãƒƒã‚¯
                     print(f"Error occurred during Step {i+1}. Stopping DSRC process.")
                     await discord_ui.delete_thinking_message()
                     error_reason = "Unknown error"
                     # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®assessmentã‹ã‚‰ã‚¨ãƒ©ãƒ¼ç†ç”±ã‚’æ¢ã™
                     for a in all_assessments: # å…¨ä½“è©•ä¾¡ãƒªã‚¹ãƒˆã‹ã‚‰æ¢ã™
                         if a.get('step') == i+1 and a.get('status') == 'ERROR': error_reason = a.get('reason', 'Unknown error'); break
                     await message.reply(f"è©³ç´°æ¤œç´¢ã‚¹ãƒ†ãƒƒãƒ— {i+1} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚\nç†ç”±: {error_reason}", mention_author=False)
                     return # æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³

                # å„ã‚¹ãƒ†ãƒƒãƒ—çµ‚äº†å¾Œã«å°‘ã—å¾…æ©Ÿ (APIè² è·è»½æ¸›)
                await asyncio.sleep(1)

            # 3. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            if not all_extracted_content:
                 await discord_ui.delete_thinking_message(); await message.reply("è©³ç´°æ¤œç´¢ã®çµæœã€æœ‰åŠ¹ãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", mention_author=False); return

            # generate_dsrc_report é–¢æ•°å†…ã§è¦ç´„å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã‚‹
            final_response_text = await generate_dsrc_report(original_question, plan, all_extracted_content, all_assessments)

            if not final_response_text or llm_manager.is_error_message(final_response_text):
                 await discord_ui.delete_thinking_message(); await message.reply(f"æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå¤±æ•—: {final_response_text}", mention_author=False); return

            response_header = f"(ğŸ”¬ **DeepResearch Report** using {primary_model_name} ğŸ”¬)\n\n"

        # --- å…±é€š: æœ€çµ‚å¿œç­”é€ä¿¡ ---
        await discord_ui.delete_thinking_message()
        full_response = response_header + final_response_text

        # ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (generate_dsrc_reportå†…ã§å‡¦ç†æ¸ˆã¿)
        # source_header = "**å‚ç…§ã‚½ãƒ¼ã‚¹:**"
        # if source_header.lower() not in full_response.lower() and all_extracted_content:
        #      print(f"[{search_source}] LLM did not include sources. Appending manually.")
        #      source_list = "\n".join([f"- <{url}>" for url in all_extracted_content.keys()])
        #      full_response += f"\n\n{source_header}\n{source_list}"


        # å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ†å‰²é€ä¿¡ (message.reply ã‚’ä½¿ç”¨)
        if len(full_response) > 2000:
            print(f"[{search_source}] Final response length ({len(full_response)}) exceeds 2000. Sending in chunks.")
            response_chunks = [full_response[i:i+1990] for i in range(0, len(full_response), 1990)]
            first_chunk = True
            try:
                for chunk in response_chunks:
                    if first_chunk:
                        # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯é€ä¿¡æ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
                        final_sent_message = await message.reply(chunk, mention_author=False)
                        first_chunk = False
                    else:
                        # 2é€šç›®ä»¥é™ã¯ãƒãƒ£ãƒ³ãƒãƒ«ã«ç›´æ¥é€ä¿¡ (Replyã«ãªã‚‰ãªã„ãŒã€ä¼šè©±ã®æµã‚Œã¯ç¶­æŒ)
                        await message.channel.send(chunk)
                    await asyncio.sleep(0.5) # é€£æŠ•åˆ¶é™å¯¾ç­–
            except discord.HTTPException as e:
                 print(f"[{search_source}] Error sending chunked final response: {e}")
                 if not final_sent_message: # æœ€åˆã®é€ä¿¡ã§å¤±æ•—ã—ãŸå ´åˆ
                      await message.channel.send(bot_constants.ERROR_MSG_INTERNAL + " (å¿œç­”é€ä¿¡å¤±æ•—)")
        else:
            # 2000æ–‡å­—ä»¥ä¸‹ã®å ´åˆã¯ä¸€æ‹¬é€ä¿¡
            try:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
                final_sent_message = await message.reply(full_response, mention_author=False)
            except discord.HTTPException as e:
                 print(f"[{search_source}] Error sending final response: {e}")
                 await message.channel.send(bot_constants.ERROR_MSG_INTERNAL + " (å¿œç­”é€ä¿¡å¤±æ•—)")


        # --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°ã¨è¿½è·¡è³ªå•ãƒœã‚¿ãƒ³ (å¿œç­”æˆåŠŸå¾Œ) ---
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ãªã„ã€ã‹ã¤ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ã«æˆåŠŸã—ãŸå ´åˆã®ã¿å®Ÿè¡Œ
        if final_sent_message and final_response_text and not llm_manager.is_error_message(final_response_text):
            # 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
            try:
                print(f"[{search_source}] Updating cache for channel {message.channel.id}...")
                # mentionã‚’é™¤å»ã—ãŸå®Œå…¨ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ (ã‚³ãƒãƒ³ãƒ‰/è³ªå•å«ã‚€)
                # bot.py ã‚„ assess_and_respond_to_mention ã‹ã‚‰æ¸¡ã•ã‚ŒãŸ query_text ã‚’ä½¿ç”¨
                # query_text ãŒ None ã®å ´åˆã¯ message.content ã‹ã‚‰å†æ§‹ç¯‰ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
                user_input_for_cache = query_text # <- question_text ã§ã¯ãªã query_text ã‚’ä½¿ç”¨
                if user_input_for_cache is None:
                    # ã“ã®ãƒ«ãƒ¼ãƒˆã¯ handle_search_command ã®å‘¼ã³å‡ºã—å…ƒã§ query_text ãŒ None ã§ãªã„é™ã‚Šé€šã‚‰ãªã„ãŒã€å¿µã®ãŸã‚
                    print(f"Warning: query_text is None in handle_search_command cache update logic. Reconstructing from message.content.")
                    mention_strings = [f'<@!{message.guild.me.id}>', f'<@{message.guild.me.id}>']
                    user_input_for_cache = message.content if message.content else ""
                    for mention in mention_strings:
                        user_input_for_cache = user_input_for_cache.replace(mention, '').strip()
                    # !src, !dsrc, -nosrc, !his ãªã©ã‚‚é™¤å» ( command_handler ã® handle_mention ã«åˆã‚ã›ã‚‹)
                    user_input_for_cache = re.sub(r'\s!-?[sS][rR][cC]\b', '', user_input_for_cache, flags=re.IGNORECASE)
                    user_input_for_cache = re.sub(r'\s-nosrc\b', '', user_input_for_cache, flags=re.IGNORECASE)
                    user_input_for_cache = re.sub(r'\b!his\b', '', user_input_for_cache, flags=re.IGNORECASE).strip()

                # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯ cache_manager ã® save_cache å†…ã§å‡¦ç†ã•ã‚Œã‚‹ã®ã§ã€ã“ã“ã§ã¯å«ã‚ãªã„
                # ã‚‚ã—å°†æ¥çš„ã«æ¤œç´¢æ©Ÿèƒ½ãŒæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›ã«å¯¾å¿œã™ã‚‹å ´åˆã€ã“ã“ã‚‚ä¿®æ­£ãŒå¿…è¦
                # ä¾‹: message.attachments ã‚’å‡¦ç†ã—ã¦ user_entry_parts_for_cache ã«è¿½åŠ 

                user_entry_parts_for_cache: List[Dict[str, Any]] = []
                if user_input_for_cache:
                    user_entry_parts_for_cache.append({'text': user_input_for_cache})


                if user_entry_parts_for_cache:
                     chat_history = await cache_manager.load_cache(message.channel.id)
                     user_entry = {'role': 'user', 'parts': user_entry_parts_for_cache}
                     model_entry = {'role': 'model', 'parts': [{'text': final_response_text}]} # LLMã®å¿œç­”å…¨æ–‡
                     await cache_manager.save_cache(message.channel.id, chat_history + [user_entry, model_entry])
                     print(f"[{search_source}] Cache updated.")
                else:
                     print(f"[{search_source}] Skipping cache update because user entry parts are empty.")

            except Exception as cache_e:
                print(f"[{search_source}] Error updating cache: {cache_e}")
                import traceback
                traceback.print_exc()

            # 2. è¿½è·¡è³ªå•ãƒœã‚¿ãƒ³ç”Ÿæˆ
            try:
                 # éåŒæœŸã§ãƒœã‚¿ãƒ³ç”Ÿæˆãƒ»è¿½åŠ ã‚’å®Ÿè¡Œ
                 print(f"[{search_source}] Generating follow-up buttons...")
                 # message.channel.id ã‚’æ¸¡ã™
                 asyncio.create_task(discord_ui.generate_and_add_followup_buttons(final_sent_message, message.channel.id))
            except Exception as btn_e:
                 print(f"[{search_source}] Error scheduling follow-up button generation: {btn_e}")
                 import traceback
                 traceback.print_exc()


    except Exception as e:
        print(f"[{search_source}] An unexpected error occurred during search process: {e}")
        import traceback
        traceback.print_exc()
        await discord_ui.delete_thinking_message()
        # message.reply ã®ä»£ã‚ã‚Šã« message.channel.send ã‚’ä½¿ã† (replyã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ä¾å­˜ã™ã‚‹å¯èƒ½æ€§)
        await message.channel.send(bot_constants.ERROR_MSG_INTERNAL + f" (æ¤œç´¢å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}...)")